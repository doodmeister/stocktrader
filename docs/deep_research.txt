Forensic Audit of the StockTrader Repository
1. Architecture & Code Quality
Project Structure: The repository is well-structured, separating concerns into modules per functionality. For instance, the README outlines a clear layout with directories for utilities (utils/), candlestick pattern logic (patterns/), machine learning training (train/), Streamlit UI pages (pages/), data handling (data/), etc
github.com
github.com
. This modular design makes it easier to navigate and maintain the codebase. Each module has a focused responsibility (e.g. etrade_candlestick_bot.py for trading logic, patterns.py for rule-based patterns, backtester.py for simulation). Code Style & Best Practices: Overall, the code follows modern Python best practices:
Type Hints: Functions and methods use type annotations (e.g. get_candles(self, symbol: str, ...) -> pd.DataFrame in the E*Trade client
github.com
, and many others) which improves clarity and enables static analysis. Custom dataclasses like TradeConfig and Trade define typed fields for configuration and trade data
github.com
github.com
.
PEP8 Compliance: Naming conventions and formatting are mostly consistent (snake_case for functions/variables, CapWords for classes). Indentation and spacing look standard. There are a few very long lines (likely due to inline comments or combined statements in the raw view) that might need line-breaking, but nothing egregious.
Logging & Error Handling: There is a unified logging utility (utils/logger.py) that sets up console and file handlers
github.com
github.com
. Code uses logger.info/debug/error throughout for traceability. Many operations are wrapped in try/except blocks with meaningful error messages (e.g. API calls, model loading), which is good for robustness.
Modularity: Functions and classes are generally of single purpose and short-to-medium length. For example, the StrategyEngine class encapsulates the live trading loop and related checks in methods like _process_symbols, _evaluate_symbol, _enter_position, etc., keeping each piece of logic separate
github.com
github.com
. This separation improves readability. Where appropriate, the code leverages helper classes (e.g. Notifier for sending alerts, ModelManager for model persistence) to abstract away details.
Areas for Improvement: A few aspects of code quality could be improved:
Duplicate Code: There is some duplicated functionality that could be unified. For instance, candlestick pattern feature engineering is implemented in two places. In train/ml_trainer.py, an add_candlestick_pattern_features function iterates over the DataFrame with a loop to add pattern indicator columns
github.com
. Meanwhile, train/feature_engineering.py defines a more efficient version using Pandas rolling apply
github.com
github.com
. Consolidating these into one canonical implementation (probably the vectorized approach) would avoid divergence in how features are added.
Inconsistent Module Use: The presence of both utils/indicators.py and utils/technicals/indicators.py (as hinted by the README structure
github.com
) is confusing. Perhaps one is obsolete or meant for different contexts. If both exist, they should be clearly differentiated or, ideally, merged to a single indicators module to prevent confusion. Similarly, there’s a core/ directory listed
github.com
 that isn’t referenced in the README’s project structure overview. If core contains legacy code or fundamental classes, it should be documented; if it’s vestigial, consider removing it to streamline the repo.
Function Complexity: Most functions are straightforward, but a few could be simplified or split for clarity. For example, the StrategyEngine.run() method contains an entire trading loop with nested calls
github.com
. While it’s still readable, further abstracting portions (like isolating market-hours waiting logic or error-handling logic) into helper methods could make it even cleaner. Similarly, the backtesting Backtest.simulate() method runs a nested loop over dates and symbols
github.com
; encapsulating some sub-logic (like processing a single day’s signals) might improve readability.
Naming & Organization: In general naming is clear; however, some class and method names can be more descriptive. For instance, PatternNN is fine for a model class, but the pipeline method train_and_evaluate in MLPipeline might be more explicit as train_deep_model or similar. Also, the repository mixes usage of underscores and camel case in file names (pattern_nn_preprocessing.json vs. patterns_nn.py). Maintaining consistency (all snake_case for file names) would be more PEP8-compliant.
Use of Dataclasses and Enums: The code makes good use of @dataclass for configuration containers like TradeConfig
github.com
 and FeatureConfig/TrainingParams
github.com
github.com
. This reduces boilerplate and provides an easy way to extend configurations. An Enum is used for ModelType (though currently only one type is listed)
github.com
 – this could be useful if more model types are added. Overall, the architecture is logically laid out and the code quality is solid. The few duplications and complexity issues can be resolved with modest refactoring. Introducing stricter linting (flake8/black) and a style guide for contributors would help keep this consistency as the project grows.
2. Logical & Runtime Errors
During the audit, several potential bugs and edge-case issues were identified that could affect correctness or cause runtime errors:
PatternNN Usage in Live Trading: In the StrategyEngine (live trading bot), the code attempts to use the machine learning model by calling self.pattern_model.predict(df) during evaluation of a symbol
github.com
github.com
. However, the PatternNN class as defined in patterns/patterns_nn.py is a torch.nn.Module that does not implement a .predict() method – it only has a .forward() (via __call__) for use during training
github.com
github.com
. This means that as-is, calling pattern_model.predict(df) will raise an AttributeError at runtime. Indeed, PatternNN instances can be invoked as pattern_model(tensor) but not pattern_model.predict(...). In the backtesting code, this is handled by a separate function pattern_nn_predict() which loads the model and runs model(window) for each window
github.com
. The live trading loop likely intended to use the trained model similarly, but it currently just instantiates PatternNN() with no weights loaded
github.com
. Impact: The ML predictions in live trading are effectively non-functional in the current state, which means the strategy might be running on rule-based confirmations only. Fix: Load the trained model weights into PatternNN in the live engine. For example, integrate a ModelManager.load_latest_model call (as seen in backtest) to load the most recent .pth file into pattern_model, and call pattern_model(df_tensor) for predictions. Alternatively, wrap PatternNN in a class that implements .predict() to encapsulate the preprocessing and torch model call.
No Update of daily_pl: The StrategyEngine is configured with a max_daily_loss threshold to stop trading if daily P/L falls too low
github.com
, but the implementation doesn’t actually update the daily_pl field anywhere. The daily_pl is initialized to 0.0 and checked in _check_risk_limits
github.com
, but never modified when trades close (realized profits/losses) or as unrealized P/L fluctuates. Consequently, the condition self.daily_pl <= -self.config.max_daily_loss will never trigger after any trades, because daily_pl remains at 0. Impact: The bot will not halt even if it hits the intended loss limit, potentially allowing further losing trades. Fix: Update daily_pl whenever a position is exited. For example, in _exit_position, calculate the profit/loss: pnl = (sell_price - entry_price) * quantity and accumulate it into self.daily_pl
github.com
github.com
. Also consider resetting daily_pl at the start of a new trading day.
Profit Target Not Enforced: The TradeConfig includes a profit_target_percent (e.g. default 3%)
github.com
, but the StrategyEngine does not use this field. The code implements a trailing stop for limiting losses (and effectively securing some profit as price moves)
github.com
, but nowhere do we see logic that takes profit at +3%. As a result, the bot might only exit via stop-loss or trailing stop, potentially missing an intended profit-taking point. Fix: Introduce a check in _monitor_positions for profit target: e.g., if current_price >= entry_price * (1 + profit_target_percent) then call _exit_position (or set a flag to tighten stop). This would realize gains as configured.
Entering Trades with Fixed Size: Currently _enter_position always uses a quantity of 1 share (hardcoded)
github.com
, ignoring the risk-based sizing calculation available. There is a StrategyEngine.calculate_position_size() method that computes shares based on account risk % and stop distance
github.com
, but _enter_position doesn’t call it (it has a TODO comment about integrating risk manager)
github.com
. While not a runtime error, it’s a logical oversight: the system isn’t actually using the position sizing rules the config implies. This could lead to suboptimal or inconsistent trade sizes. Fix: Use the calculate_position_size result in _enter_position. For example: quantity = self.calculate_position_size(symbol, price, stop_price) instead of the placeholder quantity = 1. Care must be taken to determine stop_price (perhaps base it on the trailing stop initialization or some ATR multiple).
Unrealized P/L in Backtester Not Realized: The backtesting engine does not close open positions at the end of the test period. It simulates day by day, and if a position is open on the last day but no sell signal occurred, that position remains in self.positions with its value counted in final equity
github.com
github.com
, but the trade log won’t show a corresponding sell. This can make the reported total_return accurate in equity terms, but num_trades might be understated and no final sell trade is listed in the log. It also means win_rate calculation (which only counts completed trades on sell events) could be off if the last trade is open. Fix: After the simulation loop, iterate through any self.positions still >0 and “sell” them at the final price. This would properly log the trade and finalize P/L. Alternately, treat the final equity value as liquidation for metric calculations.
Backtest Position Size Limiting: The backtester’s _process_signal method enforces a position size limit (position_size_limit = risk_per_trade) and calculates max_shares accordingly, but then it only ever buys at most 1 share due to shares = min(max_shares, 1)
github.com
. This effectively ignores the capital allocation if max_shares would be larger. For example, even if risk_per_trade allows $10k and the stock is $100 (max_shares=100), it still buys 1 share. This looks like a simplification or placeholder. Impact: Backtest results will not scale positions properly, potentially underestimating returns and risk. Fix: Remove the min(..., 1) and use the calculated max_shares (or a fraction thereof). If the intent is to simulate one share trades, clarify that. Otherwise, allowing multiple shares up to the position size limit will make backtest outcomes more realistic relative to account size.
Inconsistent Signal Confirmation: In live trading, a pattern signal must be confirmed by an indicator (RSI or MACD condition) before entry
github.com
. In backtesting of the PatternNN strategy, however, the pattern_nn_predict function generates buy/sell signals purely from the model output with no secondary confirmation
github.com
github.com
. This means the backtest might be more optimistic or just different from the live strategy, which requires an extra condition. Likewise, the live strategy also filters out entering a trade if any position in that symbol already exists (no double entry)
github.com
, whereas the backtester as written would signal every time the model says “buy”, potentially entering multiple sequential positions or ignoring that you can’t buy again without a sell. Fix: To align live vs backtest, incorporate the same logic in the backtest signal generation: e.g., only mark a “buy” if model output = 1 and (optionally) some confirmation indicator in the historical data is true, and prevent new buy signals while already “in a position” for that symbol (the Backtest class currently allows only 0 or 1 position per symbol at a time by design, since it buys then waits for a sell signal). Making the strategy rules consistent will make backtest results more predictive of live performance.
Exception Handling and Propagation: In several places broad except Exception as e: are used to catch errors (for example in the main bot loop
github.com
 or backtest wrapper
github.com
). While this keeps the system running, it may also mask specific exceptions. In the training code, a broad catch logs and re-raises as a generic ModelError
github.com
github.com
 – losing the original exception type. A better practice is to catch expected exceptions (like ValueError for bad input) and allow truly unexpected ones to propagate or at least log with traceback (the training code does use logger.exception which logs a traceback
github.com
). This is a minor point, but being careful with exception breadth helps debugging.
Edge cases in Data Handling: The data fetching utility uses Yahoo Finance and E*Trade APIs. Potential issues might include:
Using a fixed interval ("5min", 5 days) in MLPipeline.prepare_dataset
github.com
 which may not fetch enough data if training needs more. If an API returns less data than requested (market holidays, etc.), the pipeline could end up with an empty DataFrame and throw a ValueError
github.com
. The code does warn and handle an empty fetch per symbol, but if all symbols fail, it raises an exception. That’s fine, just something to monitor in usage.
In data_downloader.py, if Yahoo returns an empty DataFrame or missing dates, they raise a ValueError
github.com
, which is good. The _period_from_days logic and use of .history() seem correct for Yahoo API. Rate-limiting or network errors aren’t explicitly caught here, but presumably the safe_request wrapper (if used) or a try/except around ticker.history would be beneficial to implement (currently not shown in snippet).
Token Renewal Logic: The ETradeClient handles 401 Unauthorized by attempting renew_access_token() and then re-initializing the session with fresh credentials
github.com
github.com
. One nuance: after renewing, they call get_api_credentials() to fetch what presumably are updated tokens (the security.get_api_credentials always reads the env variables for latest credentials
github.com
). This assumes the new OAuth token is somehow stored to the environment (perhaps the user updated .env or the renew response is written there). If not, there’s a logical gap – the renewed token from renew_access_token isn’t explicitly parsed or saved in the code shown. This might lead to a loop of renewal if the old token stays in env. It’s an edge case and may depend on how E*Trade’s OAuth works (some implementations renew the same token for a window). Suggestion: After renew_access_token(), update the in-memory creds or persist them. Or use the OAuth session’s token if it refreshes internally.
Overall, none of these appear to crash the application immediately (except the PatternNN .predict bug which would throw an error in live trading). Many are logical issues that could lead to strategy misbehavior or inaccurate results. Addressing these will greatly increase the reliability of both the backtesting and live trading.
3. Machine Learning & Feature Engineering
ML Model Design: The project integrates machine learning in two ways – a “classic” ML pipeline (likely for shorter-term pattern or price prediction) and a deep learning model (PatternNN) aimed at classifying candlestick patterns or trading signals. The PatternNN model is defined as an LSTM-based PyTorch neural network with configurable layers and dropout
github.com
github.com
. By default it uses 5 input features and outputs 3 classes (which correspond to Hold/Buy/Sell signals)
github.com
. This architecture (sequence processing with LSTM) is appropriate for capturing temporal patterns in candlestick data. The use of dropout and potentially multiple LSTM layers indicates an attempt to create a robust model that generalizes beyond simple rules. Feature Engineering: This is a strong point of the project – a wide array of features are generated:
Technical Indicators: If the pandas_ta library is installed, the code computes common indicators like RSI (14-period), MACD (12-26-9), Bollinger Bands (20 period), ATR, etc., and adds them as columns
github.com
github.com
. This provides the model with momentum and volatility features known to be useful in trading. If pandas_ta is not available, it falls back to a simple moving average example (and could be extended similarly)
github.com
. All new indicator columns are filled with 0 where data is not available to avoid NaNs interfering
github.com
.
Candlestick Pattern Flags: The code programmatically adds binary features for each candlestick pattern. Using the registry of patterns in CandlestickPatterns._PATTERNS, the add_candlestick_pattern_features function (in feature_engineering.py) rolls through the DataFrame and applies each pattern’s detection logic on a rolling window
github.com
github.com
. For example, if “Hammer” is one of the patterns, it will produce a column Hammer that has 1 when a hammer pattern is present at that row, else 0. This design allows the ML model to “see” if a known pattern occurred at each timestep. Notably, the implementation leverages df.rolling(...).apply() to vectorize the operation, which is efficient. (The older loop version in ml_trainer.py works similarly but less efficiently
github.com
.) The patterns themselves are well-defined – e.g., is_hammer checks the ratio of wicks to body
github.com
, is_bullish_engulfing compares consecutive candles
github.com
, etc., and all are registered via register_pattern at import time (though the registration code is not fully shown in the snippet, it’s implied by the design).
Price/Volume Derived Features: The classic ML pipeline appears to include features like rolling percent changes and rolling volatility. In ml_trainer.get_feature_columns, we see references to features such as 'close_pct_change_{window}', 'close_std_{window}', 'volume_pct_change_{window}', 'price_volume_corr_{window}', 'sma_{window}', 'macd_{window}'
github.com
. These suggest that somewhere in preprocessing, those features are computed for various rolling windows (5, 10, 20 by default
github.com
). Although the code for computing these isn’t shown in the snippet, it’s likely in a part of ml_trainer.feature_engineering() not included in our view. Given the names:
close_pct_change_N could be N-period percentage change in closing price.
close_std_N could be rolling standard deviation of close price over N days.
volume_pct_change_N similarly for volume.
price_volume_corr_N possibly the correlation between price and volume in the window.
sma_N a simple moving average (though they also mention using pandas_ta for some of these).
macd probably the MACD line at the current row (though in pandas_ta they use macd['MACD_12_26_9'] which they store as df['macd'] and the signal line as df['macd_signal']
github.com
). The get_feature_columns suggests macd for multiple windows, which is a bit odd since MACD is fixed 12,26,9 typically. Possibly those entries might be placeholders or misnamed (or computing MACD on different time scales).
Including these features means the RandomForest (or other classic model) has a rich input vector describing recent market behavior.
Target Variable: In the deep learning pipeline, the target (y) is constructed by _extract_pattern_label, which looks at the last row of each sequence and assigns 1 (buy) if any bullish pattern is present, 2 (sell) if any bearish pattern present, else 0 (hold)
github.com
. Essentially, it’s using the presence of any pattern at the current time as the label. “Bullish” or “Bearish” here likely corresponds to categories of patterns named accordingly (e.g. bullish engulfing, morning star are bullish; shooting star, bearish engulfing are bearish, etc.). This means the PatternNN is being trained to recognize when a bullish pattern is forming (and output class 1) or a bearish pattern (class 2). It’s a form of pattern recognition learning, presumably so that it might generalize and catch patterns even if they’re noisy or partial. Evaluation of this approach: It’s a reasonable idea to let the model learn the patterns, but one must ensure that the input features actually allow the model to distinguish those patterns (since the inputs include OHLCV and perhaps even the pattern flags themselves, there is a risk of trivial learning if pattern flags are included as inputs – the code tries to avoid that by dropping non-numeric and using only float columns
github.com
, which presumably excludes the pattern binary columns from inputs when forming sequences). We see in prepare_dataset that after feature engineering, they select feature_cols as all float32/float64 columns excluding symbol/timestamp
github.com
, and then create sequences of length seq_len. They do add pattern columns to the DataFrame before that
github.com
, but those columns are int (0/1). The selection df[col].dtype in [np.float64, np.float32]
github.com
 would actually exclude the integer pattern columns (since those are int64). This is subtle: it means the LSTM is not directly given the pattern flags as inputs; it has to infer patterns from raw prices/indicators, and the pattern flags are only used to create labels. That’s a clever way to avoid feeding the answer to the model while still training it on the concept. (If this interpretation is correct, kudos to the developer for that insight.) For the classic ML pipeline, the target is likely different – possibly a forward return or a class indicating whether the price went up or down after TARGET_HORIZON days (which is 1 by default
github.com
). The ml_trainer.train_model uses y = features_df['target']
github.com
. So somewhere prior, a 'target' column is set. It could be that the classic model is trying to predict something like “will the price go up by the next day?” or “does a certain pattern occur?” Not enough is shown to be certain, but a typical approach is to label 1 if price rose >0% next day, 0 otherwise, or similar. The evaluation metrics (accuracy, precision, etc.) in ml_trainer are computed with average='binary' for precision/recall
github.com
, implying a binary classification problem (likely buy vs. not-buy). If so, they should be careful that the classes are balanced or use appropriate metrics. They do print a full classification report
github.com
 and confusion matrix – that will help in judging performance. Model Training & Validation: The classic ML uses TimeSeriesSplit cross-validation (with default 3 folds) to evaluate model performance on multiple segments of the data
github.com
. This is important for time series to preserve temporal order (and they indeed use TimeSeriesSplit, which ensures training data is always before validation data). They aggregate metrics from each fold and compute mean/std
github.com
, which gives a sense of variability. Finally, they refit the model on the entire dataset and also compute metrics on that (labeled as final_metrics)
github.com
. One slight concern: they compute final_metrics on the same data the model was just trained on (since they call pipeline.fit(X, y) on all data and then predict on X
github.com
). That will typically give an overly optimistic metric (essentially training accuracy). They do have the cross-val metrics separately, so as long as the user looks at those, it’s fine – but there's a chance someone might confuse the final training score with test performance. Ideally, one would keep a hold-out test set completely unseen, but given limited data, cross-validation is an acceptable approach. For the PatternNN, the train_and_evaluate method splits data into training and validation by train_test_split(... test_size=self.config.test_size ...)
github.com
 (default test_size is likely 0.2 or 0.3 as per MLConfig not shown, but the example usage sets test_size=0.2
github.com
). They use Adam optimizer and CrossEntropyLoss for the 3-class classification
github.com
. Early stopping is implemented: patience = 3, and they break out of the epoch loop if the validation loss hasn’t improved for 3 consecutive epochs
github.com
. This is good to prevent overfitting. They evaluate the model on the validation set after training (or after early stop) to compute accuracy and confusion matrix
github.com
. One thing: the code as shown evaluates only after training all epochs or breaking – it does not monitor validation loss per epoch to decide early stopping within that loop (the early stopping in code is actually monitoring training loss improvement, not validation loss
github.com
). This could be a bug: they compare epoch_loss (which is the training loss for that epoch) to best_loss. They never calculate validation loss inside the loop. So they’re actually doing early stopping on training loss improvement, which is not ideal – usually you stop when validation loss stops improving. They do, however, compute final validation metrics after training. It might be better to compute val_loss each epoch and use that for patience. The metrics reported for the deep model are currently just accuracy (and they collect a confusion matrix in metrics dict)
github.com
. It might be beneficial to also compute precision/recall for buy/sell signals given class imbalance (lots of hold = class 0 likely). Overfitting & Generalization: The pipeline attempts to mitigate overfitting via:
Cross-validation (for RF model).
Regularization: dropout in LSTM, early stopping.
Use of scaling (StandardScaler) for RF pipeline
github.com
 ensures no feature dominates due to scale.
Time-series appropriate validation prevents lookahead bias.
However, there are still risks:
The pattern-based labeling for the deep model essentially has the model memorizing patterns present in the training window. If those patterns are well-defined by the rules, the model might just learn to replicate them (especially if pattern flags inadvertently leaked as features, but we noted they tried to avoid that). There is a risk of overfitting to the specific price sequences seen, rather than truly learning the shape – but using multiple symbols’ data (they fetch for each symbol and concatenate
github.com
github.com
) can help the model see more variety.
The RandomForest could overfit if too deep, but they cap max_depth=10 and use 100 trees
github.com
. That’s fairly reasonable. They also use min_samples_split=2 which is default; maybe a slightly higher value could reduce overfitting, but the cross-val will indicate if it’s a problem.
Integration with Backtesting & Live Trading: This is where ML meets reality:
In backtesting, as discussed, the nn_backtest.py uses the trained PatternNN model (loading the latest .pth file) to generate signals on historical OHLCV
github.com
github.com
. It slides a window of window_size (10 by default) and for each new day (or bar) it runs the model to predict hold/buy/sell. These signals are then fed into the Backtest engine to simulate trades
github.com
github.com
. The result is that one can evaluate how the ML model would have performed on historical data, including transaction costs and so on. This is critical to validate that the model is not just good in metrics but also yields profitable trading strategies. One improvement here: the signals as produced by pattern_nn_predict are raw model outputs (argmax of network). In practice, one might want to apply some filtering or threshold (for instance, require the model to be “confident” or combine with pattern rules as the README suggests). The code currently maps outputs {0,1,2} to {hold,buy,sell} directly
github.com
. It might generate a lot of signals. Possibly, adding the rule-based filter (as mentioned earlier) or using the probability (if available via model.predict_proba) to take only strong signals could improve live applicability.
In live trading, presumably the idea is to take the same trained PatternNN and run it on real-time data (the last seq_len bars) to get a prediction for each incoming bar. The StrategyEngine was structured to handle both rule-based patterns and ML (it even has a placeholder in the dashboard for “Advanced AI Trading” and “Simple Trade” pages
github.com
). Currently, as noted, the integration isn’t fully realized (model not loaded in StrategyEngine). But conceptually, after fixes, it would do something like: for each symbol on each cycle, get recent candles -> compute tech indicators -> maybe compute pattern flags -> run PatternNN to get signal -> if signal says buy and pattern/indicator confirms, then buy. This combined approach could yield fewer but higher-quality trades. It aligns with the README’s description: “Uses CandlestickPatterns to filter ML signals – overlays final buy/sell signals on chart”
github.com
.
Classic ML model integration: The code also allows backtesting a “Classic ML Model” via classic_ml_predict
github.com
. This loads a saved .joblib model (likely a trained RandomForest pipeline) and generates predictions on features from the dataframe. It looks at model.feature_names_in_ to select the same columns used in training
github.com
. If not present, it defaults to using ['close','volume'] – which is a simplification; ideally the model’s required features are known. The classic model’s predictions (preds = model.predict(features)) might output a class label per day (the code doesn’t map them to buy/sell explicitly; presumably the model itself predicts 0/1 or -1/1). They treat whatever it outputs as the signal series directly
github.com
. This suggests the classic model might be trained to output -1,0,1 or just 0/1 that they interpret as sell/hold and buy for example. In backtest wrapper, they map any signals outside -1 to 1 range to {0,1,-1} (notice in run_backtest, they do if signals.max() > 1 or signals.min() < -1: signals = signals.map({0:0, 1:1, 2:-1}) to handle the pattern NN case where 2 is mapped to -1
github.com
). So for classic model, if it was a binary 0/1, they wouldn’t need mapping; if it produced 0/1/2, they might use same mapping. It’s a bit unclear, but the infrastructure is there to test non-ML strategies (SMA Crossover, RSI, etc.) and compare them with ML strategies using the unified backtester.
Model Performance & Backtesting Results: Without actual numbers we cannot judge performance, but the code captures key metrics:
Sharpe Ratio, Max Drawdown, Win Rate, Profit Factor in backtests
github.com
 and computes them after simulation
github.com
. The profit factor calculation does a FIFO pairing of buys and sells to accumulate profits and losses
github.com
, which is quite thorough. The presence of these metrics means when the user runs a backtest for the ML strategy, they’ll get a holistic view (not just accuracy). This is great for evaluating if the model’s predictions translate to profitable trades. For instance, a model could be 90% accurate in predicting up or down, but if those 10% of wrong predictions are big losses, metrics like Sharpe or drawdown will reveal the issue.
Overfitting checks: The training process output includes a confusion matrix and classification report printed
github.com
, and cross-val mean vs. final metrics. The user is encouraged to compare these. If final (training) accuracy is much higher than cross-val accuracy, that indicates potential overfitting. The backtest is the ultimate test: if a model shows great CV metrics but fails in backtest (low profit factor, high drawdown), it might be overfitting to patterns that don’t equate to real profit opportunities after costs. The inclusion of transaction costs (commission/slippage) in the backtest ensures the model can overcome those frictions.
Candlestick Feature Correctness: The rule-based pattern features rely on the correctness of the pattern methods in patterns.py. These appear to be implemented using standard definitions (e.g., Hammer requires a small body and a long lower wick
github.com
, Engulfing looks at two candles’ opens and closes
github.com
, etc.). They use the last row (or last two rows) of the DataFrame slice provided. One thing to watch: some patterns need a certain minimum number of candles (e.g., Morning Star is a three-candle pattern). The code does handle min_rows for each pattern (the registry stores that, e.g., morning star would have min_rows=3). The rolling apply in feature engineering uses that min_rows
github.com
 so it only computes the pattern when enough bars are in the window. That’s correctly done. So technical correctness of these features seems solid. The only possible concern is performance (rolling apply over large DataFrames for many patterns could be slow, but typically acceptable given daily data or a few years of intraday – and it’s cached in Streamlit anyway). In summary, the ML aspects are well thought-out. To strengthen them further:
Ensure alignment of what the model predicts and what the strategy uses (e.g., if the model is predicting patterns, confirm that leads to profitable decisions – maybe the model could predict price direction instead).
Potentially incorporate more advanced techniques: hyperparameter tuning for RF, or using validation loss for early stopping, etc. But these are secondary to making sure the integration with live/backtest is seamless.
4. Backtesting & Live Trading Readiness
Backtesting Framework: The repository includes a custom backtesting engine in utils/backtester.py that simulates strategies on historical data in a vectorized manner. Key characteristics:
It supports single or multiple symbols (it prepares a dict of symbol->DataFrame) but the current usage in the UI is one symbol at a time (the form has a single symbol input). The code, however, is written to handle multiple symbols in parallel by maintaining a positions dict and iterating all symbols each day
github.com
.
Daily time-step simulation: It creates a date range from the min to max date in the data and iterates day by day
github.com
. For each day, it calculates the portfolio value by summing cash (capital) plus any positions’ market value
github.com
. This assumes at most one trade per day per symbol (since they immediately update position on the same day if signal triggers). In _process_signal, if a buy signal occurs and you have cash, it buys immediately at that day’s close price, and if a sell signal occurs and you have a position, it sells at that day’s close
github.com
github.com
. Essentially, signals are interpreted as “act at close of the day.” This is a common approach, though it ignores intraday fluctuations – given daily data, that’s fine. For intraday backtesting (e.g., 5min candles), this could be adapted similarly (the code doesn’t hardcode daily, it uses the DataFrame’s index which could be datetime times).
Transaction Costs & Slippage: These are included. When buying, it multiplies cost by (1 + commission + slippage)
github.com
; when selling, it reduces proceeds by the same factors
github.com
. Defaults are 0.1% commission and 0.05% slippage
github.com
, which is realistic for many retail scenarios and ensures the backtest isn’t overly optimistic.
Risk Limits: Position sizing in backtest is governed by position_size_limit which is passed as risk_per_trade. By default in the UI, “Risk Per Trade (%)” is an input and they divide it by 100 when passing to backtest (e.g., 1.0% becomes 0.01)
github.com
. This is used such that available_capital per trade = capital * position_size_limit
github.com
. However, as noted, they only ever buy 1 share currently, so the risk limit effectively just ensures you have enough cash for that 1 share * cost. In principle, if they fix the share calculation, this will prevent oversizing positions. There isn’t an explicit portfolio-level risk management in backtester (like max drawdown stop or daily loss stop), but these can be inferred from metrics.
Outputs: After simulation, it compiles a pandas Series equity curve over time
github.com
, and a list of trades (with date, symbol, side, price, quantity, fees) which it converts to a DataFrame trade log
github.com
. The metrics (total_return, sharpe, max_drawdown, etc.) are computed in _compute_metrics
github.com
 and also profit factor via _calculate_profit_factor
github.com
. These metrics cover the key performance indicators traders care about.
The backtester’s design is sound for a first implementation. Some suggestions to enhance it:
Implement stop-loss and take-profit logic in backtesting if the strategy uses them. For example, the live StrategyEngine sets a trailing stop; the backtest currently doesn’t simulate intra-period stops. If a day’s close gaps beyond a stop, the backtest wouldn’t know. One could approximate by checking if low of day < stop price, then assume stop hit at that price, etc. That level of detail might be beyond scope for now but is worth noting for accurate simulation of strategies with intraday stops.
Right now, strategies in backtester are defined as functions from df to a single float signal (buy>0, sell<0)
github.com
. This works for strategies that decide purely based on the latest data row (they call strategy_fn(df.loc[:date]) giving data up to current date and expecting a signal for that date
github.com
). More complex strategies that need to manage state (like a long/short alternating strategy) might not fit this functional form easily. But since this project’s strategies are either indicator-based or ML model-based which inherently look at recent window and output a signal, this is fine.
The backtest vs live alignment: The live trading uses market orders presumably executed immediately. The backtest assumes execution at close price of the signal bar. If in live trading the decision happens at close and you execute a market order, you’d get next day open price typically. There’s a slight discrepancy unless one is actually trading right at close or using after-hours. To better align, one might simulate next-bar execution. But these nuances can be complex; at least being consistent (e.g., if in live you plan to trade at close via a market-on-close order, then backtest is consistent). The documentation should clarify this assumption.
Parallel Backtests / Scalability: If you wanted to backtest many symbols or a long history, this engine might become slow in pure Python. However, it’s relatively lightweight operations (a daily loop and some list comprehensions). The data sets here (daily data for perhaps a few years, or intraday for a few days) are not huge, so performance is likely fine. If needed, one could vectorize the P&L calculation more, or use libraries like Backtrader or Zipline for more advanced scenarios. But for an integrated solution, this custom approach is easier to adapt to their specific needs (e.g., seamlessly plugging into Streamlit and their data fetchers).
Live Trading Readiness: On the live side with E*Trade:
The ETradeClient class abstracts the API calls and includes robust handling of API specifics (OAuth, endpoints). It validates credentials on init by hitting an account list endpoint
github.com
github.com
, which is a good sanity check that the keys are correct. It has methods for renewing tokens
github.com
, fetching market data (candles)
github.com
, and placing orders
github.com
. The implementation shows awareness of failure modes:
If a call returns 401, it tries to renew and re-authenticate
github.com
.
If rate-limited (429), it waits the suggested time
github.com
.
If server error (>=500), it retries after a short delay
github.com
.
Any other error leads to logging and raising an error after exhausting retries
github.com
.
This kind of retry logic is crucial in a live bot to handle transient issues without crashing or stopping trading.
The StrategyEngine orchestrates live trading. It checks market hours before doing anything each cycle
github.com
, which prevents accidental trading off-hours. (It uses a timezone-aware check for 9:30-16:00 ET on weekdays
github.com
.) It processes each symbol by fetching fresh candles via client.get_candles(symbol) then calling _evaluate_symbol
github.com
. In _evaluate_symbol, the logic is: if we’re not already holding that symbol, add technical indicators to the DataFrame (RSI, MACD)
github.com
, use pattern_model.predict to detect any pattern-based signal (which, as discussed, needs to be corrected to actually use a model), then require an indicator confirmation before entry
github.com
. The confirmation currently is RSI < 40 or a bullish MACD crossover for a buy signal
github.com
. This rule is a simple filter to improve odds (it’s configurable in code, though not exposed via config parameters except the require_indicator_confirmation flag in TradeConfig
github.com
 which is true by default). If everything looks good, _enter_position is called. That places a market buy via the API and records the position internally
github.com
.
The bot tracks open positions in a dict with quantity and entry price (and associated pattern that triggered it)
github.com
. It immediately sends a notification of the order via Notifier (could be email/SMS)
github.com
, which is important for monitoring. Then each cycle it also calls _monitor_positions which updates trailing stops and exits if needed
github.com
github.com
. The trailing stop logic is: set an initial stop at entry_price * (1 - max_loss_percent) (so e.g. 2% below entry)
github.com
. As price rises, if 80% of that max_loss_percent (i.e., 1.6% if max_loss=2%) above the current trailing stop is achieved, it moves the stop up (so effectively trailing by 1.6% below the new high)
github.com
. This mechanism is a bit unusual (why 80% of the loss percent?), but it does ensure the stop ratchets upward. When current price <= trailing_stop, it triggers _exit_position which sells via API
github.com
. On exit, it also notifies via Notifier and removes the position from tracking.
Safety and Compliance in Live Trading: The engine respects max_positions – it won’t enter a new trade if it’s already holding that many symbols
github.com
. It also intended to stop if daily loss exceeded (though we noted that’s not working due to daily_pl issue). On a graceful shutdown (SIGINT), it attempts to close all positions and exit
github.com
. This is a great safety feature to avoid being stuck in trades if the program is stopped.
One thing not present is any portfolio-level risk check (like total exposure or correlation between positions) beyond the simple max count of positions. Depending on user needs, that could be fine – if each trade is fairly independent and sized by ATR, it might naturally diversify risk.
Deploying Locally via Docker: The user plans to use Docker for local deployment (and indeed there’s a Dockerfile and docker-compose.yml provided
github.com
). This suggests the app can run in a container, likely hosting the Streamlit app and possibly a background process for live trading. The Docker setup likely ensures the environment (Python 3.8+, needed packages) is consistent. For live trading, connecting to E*Trade’s API from within Docker is fine as long as network is allowed. The .env file would need to be provided (Docker could use env_file: .env in compose or something similar). We should ensure:
The Docker container has a way to keep credentials secure (not baking them into the image – using an env file as they do is correct).
If using Streamlit in Docker, expose the port etc., which the compose likely does.
If the user eventually wants to paper trade or live trade continuously, running the container on a local machine or a Raspberry Pi or cloud VM should be similar. They just need to keep the process running. The code is largely ready for that, aside from the improvements noted.
Comparison to Industry Practices: Many trading bots use frameworks like Backtrader for backtesting and something like IB’s API or Alpaca for live. Here, a custom solution was built, which is perfectly acceptable and often more flexible. The key is that after thorough testing, the live strategy should behave as the backtest predicts. Any deviations (like the confirmation filters, or missed profit targets) should be reconciled. The fact that they integrated notifications and risk controls shows good preparedness for real trading – it’s not just a research toy, but meant to be a functional system. Before going live (even in a sandbox), the following should be verified:
All exceptions in live trading are caught so the bot doesn’t crash mid-day. In StrategyEngine.run(), they catch any exception in the main loop, log it, sleep 10s and continue
github.com
. That ensures a transient error (like a network glitch or a parsing error) doesn’t kill the program immediately. This is good. They might want to be careful to not catch something repeatedly that’s irrecoverable (e.g., if credentials are wrong, it will log error every loop). But since they validate on start, that’s less likely.
The E*Trade API credentials – the code expects them in env and loaded via security.py at startup
github.com
. In Docker, these env variables need to be passed in. The docker-compose.yml probably handles that by referencing .env. Just double-check that .env includes all needed keys (consumer key/secret, OAuth token/secret, account ID, etc.), and none of those are checked into version control (only .env.example is, which is correct practice).
Time synchronization: The bot uses local system time (with an offset to Eastern Time) to decide market open/close
github.com
. If the server clock is wrong, this could be an issue. Typically, one ensures the Docker host has NTP sync. Probably fine, but worth mentioning.
In summary, after addressing the noted issues (especially loading the ML model properly and fixing the risk tracking), the system is close to live-trading readiness. It’s already capable of backtesting strategies to refine them. The transition from backtest to live is straightforward conceptually: the same logic is applied, just data comes from API in realtime instead of a historical CSV. The consistency between the modules (using identical functions where possible) will ensure what you tested is what you trade.
5. Performance & Scalability
With the current scope (a handful of symbols, moderate historical data, single-threaded operation), the project performs adequately, but there are some bottlenecks to consider if scaling up:
Data Fetching Loops: In live trading, the bot fetches data for each symbol sequentially every polling_interval (5 minutes by default)
github.com
github.com
. If the number of symbols grows large, doing these one by one could start to take significant time (especially if API calls sometimes take a second or two each). For example, monitoring 50 symbols with a 5min interval might not complete the cycle before the next interval starts. A possible improvement is to fetch data in parallel or in batches. Since the ETrade API likely doesn’t have a batch endpoint for multiple symbols (the code calls /market/quote/{symbol}/candles per symbol
github.com
), one approach is threading or async IO to overlap the requests. Python’s GIL might limit pure threading benefits, but using asyncio with an HTTP client or concurrent futures with requests could speed up multi-symbol refreshes. Given the current scale (maybe a dozen symbols max, typical for a single trader), this is not urgent. But it’s something to note for scalability. Additionally, caching any data that doesn’t need refetch (the bot currently always pulls the last X candles anew – ETrade might not offer a “latest since” endpoint easily, so it re-downloads overlapping data).
Indicator Calculations: The technical indicators are computed on each new data fetch in live (via TechnicalIndicators.add_rsi, etc.)
github.com
. If these functions are efficient (likely using numpy/pandas under the hood), it’s fine. If not (e.g., computing a long RSI from scratch each time), we could optimize by maintaining a rolling window of last N bars and updating the indicator incrementally. But again, given the polling frequency and small data slices (5 days of 5-min candles in the ML pipeline example
github.com
, or maybe a few months of daily in other cases), this overhead is small.
Machine Learning Inference: Running the LSTM model for each new bar is very fast on a modern CPU or especially if using a GPU (which the code can, since it moves model to self.device GPU if available
github.com
github.com
). The PatternNN has only ~64 hidden units and 2 layers by default, which is lightweight. The backtest, however, currently does many forward passes: it loops through each index of the DataFrame and does model(window) for each step
github.com
. If you had, say, 1000 time steps, that’s 1000 forward passes. This is not too bad (and with 1000*5 bars of input, still fine), but it’s not vectorized. A more vectorized approach could be to run the model on the entire sequence in one go. For example, one could reshape the input as (1, seq_len, features) and slide a window across it using convolution or simply loop in PyTorch (which might be marginally faster in C than in Python). However, given typical daily data lengths (a few hundred data points for a year), this optimization is low priority. If one were to backtest on intraday data with tens of thousands of points, then vectorizing or batching the prediction calls would become important.
Python Loops in Backtester: The backtesting loop is Python-native. Each day it potentially loops through symbols and does a constant amount of work. The heavy lifting inside (calculating returns, etc.) is done using pandas operations at the end for metrics, which is fine. If we wanted to simulate tick-by-tick or very fine-grained data, a Python loop might become the bottleneck. But for daily or even hourly bars, it’s acceptable. There is an opportunity to vectorize the core logic: for example, one could precompute signals for all days (which we actually do, since signals is a Series across dates
github.com
), and then vector-multiply by positions to compute P&L series. But because they allow partial position (carry position, accumulate P&L over time), a loop is easier to manage logically. As long as the dataset isn’t huge, clarity trumps micro-optimization here.
Memory Usage: DataFrames for even a decade of daily data are small (a few KB). If the user loads very large datasets (minute bars for many years across many symbols), memory could spike. There is no explicit caching of historical data between backtests (each backtest call fetches fresh data via load_ohlcv which in this implementation generates random data
github.com
 but in reality would load from disk or an API). One might want to implement a caching mechanism for historical data in real use – e.g., store downloaded Yahoo data in data/ folder and reuse it for repeated backtests on the same period. This is an enhancement to consider if backtesting becomes slow due to repeated data fetch.
Parallelizing Training: Training the RandomForest is already parallelized internally (n_jobs=-1 uses all cores for the trees
github.com
). The cross-validation loop could be parallelized (train each fold in separate thread), but since each fold training itself can use all cores, it’s fine as is (parallelizing at two levels might actually oversubscribe CPU). The LSTM training is single-threaded on CPU (or uses the GPU if available). The dataset sizes are not huge so epoch times likely in seconds. If in future one were to try many hyperparameter combos or run very long sequences, they might consider more advanced training regimes or distributed setups, but not necessary now.
Continuous Operation: One performance aspect is running the live bot continuously. Python/Streamlit can handle long-running processes, but if using Streamlit’s UI to also trigger things, one must ensure the background loop doesn’t stall the UI thread. It looks like the design is such that the Streamlit app is mainly for dashboards and launching things (and backtests), whereas the actual live trading might be intended to run as a separate script/process (perhaps utils/etrade_candlestick_bot.py has a main() to run the StrategyEngine
github.com
github.com
). Indeed, at the bottom of that file they have a main() that instantiates client and engine from env config and calls engine.run()
github.com
github.com
. So you could run python utils/etrade_candlestick_bot.py in a terminal (or as a service in Docker) to execute the live strategy loop, without the Streamlit UI. The UI is more for monitoring and manual control. Running these concurrently is possible (the UI could even spawn the bot thread if designed, but likely better separate). This architecture is scalable: one container/process for the Streamlit dashboard, one for the live trading bot. They both read the same data directory (for models, etc.) and same env. This separation means the UI responsiveness won’t be affected by trading loop and vice versa.
CI/CD and Testing Performance: It’s worth noting that if they integrate automated tests, they should be mindful that heavy backtests or model training in tests can slow down CI. They can mitigate this by using smaller data or mocking parts of it. Given the focus on local development, one might set up a Continuous Integration pipeline that runs linting and tests on each commit. The .github/workflows likely contains something (maybe running pytest). Ensuring those tests run quickly (maybe skip actual API calls or large backtests) will keep iteration speed fast.
Optimizations Recap: To summarize practical improvements:
Use Streamlit’s caching more extensively. They already use @st.cache_resource for loading models so that you don’t reload on every interaction
github.com
. They could cache data loads as well if hitting Yahoo frequently (though data can change if backtesting new ranges, so maybe not trivial).
Possibly use a job queue or multi-threading if one wanted to backtest multiple strategies in parallel. Currently, one backtest runs at a time (triggered by form submit). That’s fine for interactive use.
For scalability to more symbols or strategies, consider structuring code to allow adding new strategies easily (the STRATEGIES dict in backtester is a good start
github.com
). They can simply add a new strategy function and plug it in. That’s more about maintainability, but it also means you can test many strategies systematically by iterating that dict, which could be something to do offline (maybe a future feature: a batch backtest mode comparing strategies).
Overall, the current performance is likely more than sufficient for its intended use. There are clear places to optimize if requirements grow, but “premature optimization is the root of all evil” – so the code rightly prioritizes clarity and correctness, with heavy computations already done in optimized libraries (pandas, numpy, scikit-learn, PyTorch).
6. Security & Compliance
Security in this context mainly involves protecting sensitive information (API keys, account access) and ensuring the system doesn’t accidentally do something unsafe (like send unintended orders or expose vulnerabilities). API Credential Management: The application handles secrets appropriately:
API keys and tokens for E*Trade are never hard-coded; instead they are loaded from environment variables via dotenv. An example .env.example is provided to show what’s needed
github.com
. The utils/security.py automatically calls load_dotenv() on import to load these into os.environ
github.com
, and provides helper functions to retrieve them (get_api_credentials() and get_openai_api_key())
github.com
. This means that in production, the user should supply a real .env file, which is .gitignored, so keys won’t leak into version control. This is standard good practice and it’s being followed.
The code does not print or log the actual credentials (it only logs errors like “Missing credentials”
github.com
). So the risk of accidentally exposing keys in logs is low. Just ensure that any added debugging doesn’t log those values.
Network Calls & Data Security: All external communication is through secure channels:
E*Trade API calls use HTTPS endpoints (noted by the base URLs apisb.etrade.com or api.etrade.com)
github.com
. The requests library by default will verify SSL certificates, so the data in transit is encrypted.
Yahoo Finance data via yfinance also uses HTTPS under the hood.
Notifications: If using SMTP, Twilio, Slack – one must also provide credentials/tokens for those. The design likely expects those in .env as well (not explicitly shown, but probably fields like SMTP password, Twilio SID, etc.). Similar care should be taken for those. Given Notifier exists, it probably reads env variables for these services. We didn’t open utils/notifier.py, but it's worth checking briefly if there are any secrets or if it’s sending to webhooks which should be kept confidential. In any case, the same .env approach should apply.
Injection and Execution Risks:
Streamlit UI: Streamlit is generally safe from code injection, since it doesn’t expose a typical web form processing (it’s Python run, not e.g. SQL queries). The user inputs like stock symbol or dates are used in the code (symbol = st.text_input(...)). There is a validate_symbol function in data_downloader.py that likely sanitizes symbols (ensuring they are alphanumeric/tickers)
github.com
. Indeed, they do sanitized = validate_symbol(symbol) in a try/except and skip if invalid
github.com
. This prevents someone from, say, inputting a malformed symbol that could break an API call or be used maliciously (though the vector for malicious use is limited – at worst a weird symbol could cause a heavy download or crash).
Pattern Editor: The ability to edit candlestick pattern code via the UI (patterns.py editing through pattern_utils.write_patterns_file) is powerful but poses a security risk if used improperly. Essentially, a user can inject arbitrary Python code into the patterns.py by using the editor page (since it likely takes text input for a new pattern function). This is not a typical injection (the user themselves is writing code to customize the bot). If you run this in a multi-user environment or as a hosted service, it would be dangerous (someone could write malicious code to steal keys or crash the system). In a controlled local environment, it’s more of a feature than a vulnerability – you trust yourself to only write safe patterns. The design, however, should be careful: they provide validate_python_code(code) using ast.parse to do a syntax check
github.com
, but that doesn’t prevent logic bombs. There’s not a simple fix here except not exposing that feature publicly. For local use, it’s okay. Perhaps add warnings or restrict what patterns can do (like run them in a sandbox, which is non-trivial in Python). For now, treat this as a power-user feature and document it clearly.
Dependencies: All third-party libraries should be kept up to date to patch any known vulnerabilities. For example, requests or pandas occasionally have security patches (though rare for those). The requirements files should pin versions to known-good ones (the presence of requirements-dev.txt and requirements.txt
github.com
 indicates some effort in environment management). One might run safety checks or use GitHub Dependabot to monitor these. Using pip install -r requirements.txt as instructed
github.com
 means you get whatever versions are in that file, so if they pinned older versions, that could be an issue if those have vulnerabilities. We would review requirements.txt if we had it (likely includes Streamlit, sklearn, torch, etc.). For instance, older Streamlit had an issue with open redirect, but nothing severe. PyTorch had some potential security issues in model loading (if loading untrusted models, which is not the case here since you train your own). Just be mindful if any library announces a security flaw.
Compliance (Financial): If this bot is used in live trading, the user should be aware of trading regulations:
Pattern Day Trading Rule: In the US, if the account is under $25k and does 4 or more day trades in 5 days, it can be flagged. The bot could easily execute multiple day trades (particularly if running on short intervals). The code does not track whether a trade is a “day trade” (buy and sell same day) or attempt to limit them. If the user’s account is small, they should adjust the strategy to avoid day trades or only trade on longer intervals (or simply be mindful and possibly turn off the bot if it’s about to violate rules). This is outside code enforcement for now, but a note in documentation could be helpful (e.g., “Ensure compliance with your broker’s day trading rules. This bot does not automatically prevent pattern day trading.”).
Order Handling: The code places market orders. Market orders can have slippage (which they simulate in backtest) and no guarantee of price. A cautious approach in volatile markets is to use limit orders. But that adds complexity (you’d have to decide the limit price, monitor if filled, etc.). Using market orders is simpler and likely fine for highly liquid stocks, but one must ensure the bot doesn’t send orders in illiquid symbols that could cause huge slippage. Perhaps a future safety check: average volume or spread checks before trading.
Logging and Audit: For compliance and debugging, logging all actions is good. The bot logs every order placed and position closed with info level
github.com
github.com
. The Notifier sends out trade notifications which creates an audit trail outside the system (e.g., to email). This is helpful for later analysis or proving what was done if needed. The trade logs from backtesting can also be used as a baseline expectation.
Privacy: Since this is not a multi-user system, privacy concerns are minimal. The only personal info might be account IDs or phone numbers/emails for notifications, all of which should be kept in env variables or config, not shared.
System Security: Running this bot on a local machine or server means typical security of that environment matters. Docker can help encapsulate it. One should keep the OS updated and restrict access to the machine or container. The bot code doesn’t expose any network service (except Streamlit’s web interface which, if left open on the internet, should be protected – by default Streamlit is not secure or authenticated. If deploying the dashboard, consider running it in a secure network or requiring a password using Streamlit’s authentication features or simply not exposing it publicly). OpenAI API Key: There’s a mention of get_openai_api_key() in security.py
github.com
. Possibly the “Advanced AI Trading” page or some feature uses GPT for analysis (maybe generating commentary or suggestions). That key is also loaded from env. If using it, same care applies. Also, be mindful of not sending sensitive data to OpenAI inadvertently. But since it’s likely an optional feature, we won’t digress. In summary, the repository demonstrates good adherence to security best practices for a trading bot:
Secrets management via env files,
Cautious API usage with error handling,
No obvious injection points (besides intentional code editing feature),
Simulation of adverse scenarios (slippage, etc.) in backtest to avoid overconfidence.
Addressing the few mentioned items (like documenting the pattern editor risk, ensuring proper use of credentials after token renew) will further solidify security. Always test with a paper trading account or sandbox (E*Trade provides a sandbox mode, which they utilize via the sandbox=True option
github.com
 tied to ETRADE_USE_SANDBOX env) before going live to catch any unintended behaviors.
7. Documentation & Maintainability
Documentation:
README & User Guide: The README (titled “E*Trade Candlestick Trading Bot & Dashboard Manual”) is extensive and well-organized
github.com
. It covers key features, installation steps, configuration, project structure, usage examples, and more
github.com
github.com
. This provides an excellent starting point for new users and contributors:
It enumerates the technical and ML capabilities (technical indicators, patterns, backtesting metrics) clearly
github.com
github.com
.
Installation instructions are given for both Windows and Unix-like systems
github.com
, which is thoughtful.
The Project Structure section explicitly lists each important file and its purpose
github.com
github.com
, which greatly helps newcomers understand where to find things.
Usage examples show how to run the dashboard and various scripts (though there is a slight mismatch as noted below)
github.com
github.com
.
There’s a Machine Learning Pipeline section describing the end-to-end flow from data download to signal generation
github.com
github.com
. This is useful to conceptually understand how all parts come together.
A “Further Improvement Suggestions” section exists in the README
github.com
github.com
, indicating the authors are aware of some tasks to be done (like splitting docs, adding more examples, etc.).
One discrepancy: the usage instructions mention commands like python patterns.py --symbol ... and python backtest.py --strategy ml ...
github.com
, but in the repository, there is no standalone backtest.py (backtesting is through pages/nn_backtest.py or utils/backtester). Perhaps they intended to create a CLI script for backtesting but currently, one would use the Streamlit UI or write a small script to call run_backtest. Similarly python training/train_model.py vs the actual structure. This is a minor documentation inconsistency. It would be good to update the README usage section to match the actual implementation (e.g., instruct to run streamlit_dashboard.py or streamlit run ... as they do, and clarify how to run backtests through the UI or a provided function).
In-Code Documentation: Most modules and many functions include docstrings that explain their purpose and usage. For example:
The top of patterns.py has a descriptive docstring outlining its purpose and design choices
github.com
.
Each pattern detection function has a docstring describing the candlestick pattern in plain language (visual, psychology, significance)
github.com
github.com
 – this is excellent documentation for someone who may not know what a “Morning Star” or “Bullish Engulfing” means. It turns the code into an educational resource as well.
etrade_candlestick_bot.py has a module docstring explaining it’s the main engine and how pattern detection is separated out
github.com
.
Critical classes like BacktestConfig and BacktestResults have concise docstrings in the code or comments explaining fields
github.com
github.com
.
Functions like empty_backtest_result, load_ohlcv, etc., have docstrings that clarify what they return or do
github.com
github.com
.
There are some functions without docstrings (perhaps for brevity or obvious ones). E.g., _process_signal in backtester has an inline comment but not a formal docstring
github.com
. In general, though, the important parts are covered. The Google-style docstrings (parameters and return types) are not consistently used; some just have a narrative description. That’s fine, but ensuring each public-facing function at least has a one-line summary would be ideal.
Comments: The code includes explanatory comments especially where logic might be non-obvious:
For instance, in _calculate_profit_factor, a comment explains it’s doing FIFO pairing of trades
github.com
.
In MLPipeline.prepare_dataset, comments mark where feature engineering happens and why certain checks are in place (e.g., “# Ensure X is 3D” checks with error logs)
github.com
github.com
.
The pattern functions’ comments double as documentation (embedded in docstring as multi-line comment actually).
We could add more comments in complex math parts, but it’s already fairly clear.
Maintainability:
Code Organization: As discussed under Architecture, the modular breakdown helps maintainability. Each module can be modified with minimal impact on others (for example, one could swap out the RandomForest in ml_trainer with another algorithm without touching the backtester or the Streamlit UI). The clear separation also means future contributors can find where to add new features: e.g., add a new pattern -> edit patterns.py; add a new strategy -> either write a function and add to STRATEGIES or create a new Streamlit page.
Testing Framework: There is a tests/ directory indicated
github.com
, but we haven’t seen specific test files. If tests are implemented, they’re crucial for maintainability because they ensure changes don’t break existing functionality. The README mentions running pytest --cov=stocktrader
github.com
, implying tests do exist. Increasing test coverage is always beneficial. Areas that should be tested thoroughly:
Pattern detection (feed known OHLC patterns and assert the functions return True).
Indicator calculations (perhaps compare a known RSI output to the function’s output).
Backtest edge cases (no signals should result in no trades and zero return, a sequence of buy then sell signals yields expected P&L).
ML pipelines could be smoke-tested (train on a tiny artificial dataset to ensure no exceptions).
API integrations might be hard to test without sandbox keys; perhaps they can mock the ETradeClient for offline testing of the strategy logic.
If not already done, setting up continuous integration to run tests on each push would catch regressions early. The presence of .github/workflows suggests they planned this.
Extensibility: The design choices indicate foresight for extension:
Using a config class (MLConfig) for ML parameters suggests you can tweak settings or run different experiments easily (like change seq_len, epochs, etc. in one place).
The ModelManager likely abstracts saving/loading models so one could implement different storage (local vs cloud) without changing training code.
The Notifier is abstracted to handle multiple channels, so adding a new notification method (say Telegram) would be done by extending Notifier, not touching core logic.
The pattern registry allows adding new candlestick patterns by just writing a function and registering it. This is very maintainable as new patterns emerge or the user wants to custom define them.
The backtester strategy registry likewise allows adding strategies easily. For example, if one wanted to test a new indicator strategy, just implement a function and add to the dict, voila it appears in the UI select box (the UI already populates the selectbox from the list including “SMA Crossover”, “RSI Strategy”, etc.
github.com
).
Comments on Docker/Deployment: For maintainers, having Docker means environment differences are less of an issue (“works on my machine” problems are reduced). The Dockerfile needs to be kept up to date with the requirements. Also, as new versions of Python or libraries come out, one should update and test. It’s good to pin versions in Docker to known stable ones.
Areas to Improve Documentation:
As mentioned, correct the README usage section to current scripts (maybe provide a CLI entry point for backtesting if that’s easier).
Add a section on how to retrain the ML model and deploy it: It’s somewhat covered (the README’s usage step 4 "ML Model Training" shows a command
github.com
, but ensure that script exists and explain where the model gets saved, and that the live dashboard will automatically pick up the latest model if named appropriately).
Possibly add an Architecture Diagram or flowchart in the docs (maybe in the docs/ folder or wiki) to visually represent the system. The README’s textual description is good, but a diagram could help newcomers (for example, a flow of: Data -> Feature Engineering -> Model -> Signal -> Backtest -> Live Trading loop).
Provide more example scenarios in documentation: e.g., “To run a backtest of the PatternNN strategy on AAPL for last year, do X (or use UI as follows) and expect output Y.” This manages user expectations and showcases usage. The README already has some examples, so building on that is straightforward.
Docstrings for tricky functions: ensure functions like _extract_pattern_label (which has an important role) are fully documented with assumptions. Right now it has a brief comment but could explicitly say “Looks for any bullish pattern column ==1 in the last row of the sequence to assign a BUY label.” Being explicit helps if someone later changes feature engineering (e.g., if they rename pattern columns or include new ones, they know to update this logic accordingly).
Given the plan for local Docker deployment, a short section in docs about “Running with Docker” would be useful (maybe it’s in a separate docs file). It might outline how to build the image, how to supply the .env, how to access the Streamlit UI, etc. This ensures even non-experts in Docker can get it running.
Maintenance Roadmap: The maintainability is bolstered by the fact that the authors themselves left improvement suggestions. Combining those with our findings:
They suggested splitting documentation for users vs developers, which is a good idea as the project grows (e.g., have a user manual for how to use the UI and configure the bot, and a developer guide for how to extend or modify the system).
They likely want to add more code examples for each dashboard page – this would help both as usage demos and as tests (examples serve as informal integration tests).
Code style enforcement (maybe they plan on adding a linter config or formatting). Setting up a pre-commit hook for black/flake8 could keep contributions clean.
All considered, the project is in a good state documentation-wise for its current stage. Continued diligence in updating docs with code changes, writing tests for new features, and refactoring duplicated code will ensure it remains maintainable as complexity grows.
Having analyzed all these dimensions, the repository demonstrates a strong foundation. The next section outlines a prioritized action plan to address the issues and enhancements identified, which will guide maintainers on improving the project further.
Prioritized Next Steps & Recommendations
Taking into account the findings above, here is a recommended roadmap for improvements, roughly in order of priority:
Fix Critical Bugs and Inconsistencies:
a. PatternNN Integration: Modify the live trading code to properly load and use the trained PatternNN model. Implement a predict() method or use the existing backtest prediction function to avoid the current AttributeError
github.com
. Ensure pattern_model in StrategyEngine is loaded with weights (e.g., via ModelManager.load_latest_model) before use.
b. Risk Management Variables: Update the StrategyEngine to track and use profit/loss. Increment daily_pl on each position exit (realized P/L) and reset it daily. Enforce max_daily_loss by stopping new trades or shutting down trading when the threshold is hit
github.com
. Also utilize profit_target_percent by adding logic to take profit in _monitor_positions or as a limit-order equivalent.
c. Quantity Calculation: Use the calculate_position_size() output in _enter_position instead of the hardcoded 1 share
github.com
. This might involve deciding on a stop-loss price (for risk per trade calculation) – possibly use the initial trailing stop as the worst-case loss. This change will allow positions to scale with account size and risk.
d. Backtest Closing Positions: Amend the backtester to close any open positions at the end of simulation. This could be done after the date loop – iterate through bt.positions and for any symbol with position > 0, simulate a sell on the last date’s close (or last available price). Add those to the trade log and reflect in final equity. This ensures metrics like total_return and num_trades account for that final position.
Refactor and Clean Up Code Duplication:
a. Unify Feature Engineering: Eliminate the duplicate add_candlestick_pattern_features in ml_trainer.py
github.com
 in favor of the feature_engineering.py version
github.com
. For example, have ml_trainer import and use train.feature_engineering.add_candlestick_pattern_features (which is more efficient). This prevents divergent behavior if one is updated. Likewise, ensure only one indicators.py is the source of truth for technical indicators (if utils/indicators.py is redundant due to utils/technicals/indicators.py, remove or consolidate them).
b. Streamline Configuration: There are multiple config definitions (TradeConfig, MLConfig, FeatureConfig). Review if some can be consolidated or if all are necessary. For instance, FeatureConfig and what’s inside MLConfig might overlap. A single config.yaml (as hinted in usage) could define all settings and be loaded for consistency. Using dataclasses or Pydantic models for all configurations (some parts already use Pydantic for BacktestConfig
github.com
) can enforce validity and simplify passing configs around.
c. Rename/Migrate Legacy Elements: If the core/ directory or any outdated scripts (like backtest.py, train_model.py) exist, clarify their status. Remove them if they’re deprecated, or update them to call into the new modular functions. This avoids confusion for maintainers stumbling on multiple ways to do the same thing.
Enhance Testing and CI Pipeline:
a. Expand Test Coverage: Write unit tests for all candlestick pattern functions (simulate known candle setups and assert detection). Test the technical indicator calculations against a library or known result to ensure correctness. Add tests for the backtester (e.g., feed a static signal series and verify metrics outcomes – you can craft a scenario where you know what the win rate or Sharpe should be). Test the ML pipeline on a tiny dataset (even random data) to ensure the training functions run without error and improve metrics when expected.
b. Integration Test Backtest vs Live Logic: Consider a “dry-run” mode for StrategyEngine that doesn’t call the API but uses recorded data and decisions, then verify that for a given historical period, StrategyEngine’s decisions match what the backtester does for the same period. This could be complex to implement but would be the ultimate regression test: ensuring that refactoring doesn’t make live logic deviate from what was tested in backtest.
c. Continuous Integration: Utilize GitHub Actions (the workflow file exists) to run tests and linters on each push/PR. Integrate code quality tools: flake8/Black for style, mypy for type checking (since type hints are present), and perhaps safety or dependabot for vulnerabilities. This automated feedback will maintain the project’s quality as new contributions come.
Improve Backtesting Realism & Flexibility:
a. Slippage/Commission Model: Currently fixed percentages are used. This is fine, but consider allowing these as parameters in backtest config (they already are fields in BacktestConfig
github.com
 but always set to default in code). Expose them in the UI (e.g., user could input 0.001 or 0 if they want to see ideal scenario).
b. Intraday Backtesting: If planning to simulate strategies on intraday data (5min bars), modify the backtester to iterate over DateTime index (not just date) properly. It might already handle it if freq is fine enough, but ensure the date range generation and index checks in simulate() work with intraday times. Possibly switch to iterating over df.index rather than pd.date_range for full generality (this way it naturally covers each timestamp present).
c. Multi-symbol strategies: If future strategies need to trade multiple symbols based on combined logic (e.g., rotation strategies), the current design might need extension. For now, each symbol is independent in backtest and live, which is okay. Just keep in mind if adding portfolio-level strategies, backtester might need to call strategy_fn with data from all symbols at once. This could be a future project milestone.
Documentation and Usability:
a. Update and Split Documentation: Refresh the README usage examples to match the current code (if backtest.py doesn’t exist, change to instruct using the UI or provide a small snippet on how to call run_backtest_wrapper from a Python prompt). Add a section on running the live bot (e.g., “To start live trading, run python utils/etrade_candlestick_bot.py after filling .env with your API keys. Monitor logs and use the Streamlit dashboard for real-time visualization.”). Split advanced developer info (like extending patterns or strategies) into the docs/ folder or wiki, as planned.
b. Diagram and Examples: Include a high-level architecture diagram illustrating data flow (could be in docs/architecture.png and referenced in README). Also add example notebooks or scripts in examples/ showcasing common tasks: e.g., “train a new model on historical data CSV,” “run a quick backtest via code,” etc. This will help users not using the Streamlit UI.
c. User Warnings and Tips: Document the pattern day trading consideration and encourage sandbox testing. Also, mention that the “Advanced AI Trading” page is experimental (if it uses OpenAI API, outline what it does and any cost implications). Provide guidance on parameter tuning – e.g., “the default ML model may overfit on short windows; experiment with seq_len or different indicators if performance is lacking.” These tips can save users time and align expectations.
d. Code Comments: As you refactor, ensure any complex logic added (like the new profit-target or daily loss cut mechanism) is commented clearly so future maintainers understand why it’s there.
Performance Optimizations (as needed):
a. If the number of symbols or frequency of data increases, implement asynchronous fetching for live data (using Python’s asyncio or multi-threaded requests). This could potentially cut down waiting time in each cycle. Test the throughput of E*Trade API – ensure you don’t hit rate limits; if you do, perhaps throttle requests or fetch multiple symbols per API call if available.
b. Monitor the Streamlit app’s performance – too much data in the equity curve could slow it. They already limit default backtest to 1 year. If users select a very long range, possibly down-sample for plotting or paginate the trade log.
c. As an enhancement, one could incorporate vectorized prediction for PatternNN as mentioned (predict all time steps in one go). This is lower priority, but could be done after all functional fixes.
New Features and Enhancements: (These are lower priority than fixing and solidifying the current system, but worth planning for once the core is stable)
Implement additional strategy types (e.g., allow combining PatternNN with rule-based strategy in backtest to mimic the live filtering logic, or add other ML models like an SVM or a different deep model for comparison).
Expand the notification system to include more granular alerts (e.g., daily summary of trades, which the code has a stub for in send_daily_summary
github.com
 but isn’t called; you could call this at end of day in StrategyEngine and include P/L info once daily_pl is tracked).
Docker/Deployment improvements: possibly create a Docker image that runs both the Streamlit and trading loop (though separating them is cleaner). Provide a docker-compose that starts the bot and the UI as separate services. This would simplify launching the whole system.
Web interface security: If exposing Streamlit externally, consider an authentication mechanism so not everyone can access your dashboard or, worse, execute the pattern editor. Streamlit doesn’t have built-in auth, but running it behind a proxy with basic auth or generating static reports for non-interactive viewing could be solutions.
By tackling the items above in order, the project will become more robust, easier to use, and easier to extend. Start with the bug fixes to ensure the system behaves correctly, then move on to refactoring and tests for long-term health, and finally polish documentation and add features once the foundation is solid. This approach will incrementally improve the StockTrader bot and facilitate its transition from development to a reliable trading tool ready for real-market deployment.