"""Production-Grade Training Pipeline

Train machine learning models based on historical OHLCV data (CSV) generated by data_loader.
Includes robust error handling, logging, validation, and modular design.
"""

import os
import logging
from pathlib import Path
from typing import Optional, List
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.exceptions import NotFittedError
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

# --- Configuration ---
DATA_DIR = Path("data/")
MODEL_DIR = Path("models/")
MODEL_DIR.mkdir(exist_ok=True)

LOG_FILE = MODEL_DIR / "training_pipeline.log"

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("training_pipeline")

# --- Constants ---
REQUIRED_COLUMNS = ['Open', 'High', 'Low', 'Close', 'Volume', 'date']
FEATURES = ['Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'volatility', 'sma_5', 'sma_10']
MIN_SAMPLES = 100

# --- Utility Functions ---

def validate_file_path(file_path: Path) -> None:
    """Validate that the data file exists and is readable."""
    if not file_path.exists():
        logger.error(f"Data file {file_path} does not exist.")
        raise FileNotFoundError(f"Data file {file_path} does not exist.")
    if not file_path.is_file():
        logger.error(f"Data path {file_path} is not a file.")
        raise ValueError(f"Data path {file_path} is not a file.")

def validate_dataframe(df: pd.DataFrame) -> None:
    """Validate input DataFrame for required columns, types, and size."""
    missing = set(REQUIRED_COLUMNS) - set(df.columns)
    if missing:
        logger.error(f"Missing required columns: {missing}")
        raise ValueError(f"Missing required columns: {missing}")
    if len(df) < MIN_SAMPLES:
        logger.error(f"Insufficient data: {len(df)} rows (minimum {MIN_SAMPLES})")
        raise ValueError(f"Insufficient data: {len(df)} rows (minimum {MIN_SAMPLES})")
    if df[REQUIRED_COLUMNS].isnull().any().any():
        logger.error("Null values detected in required columns.")
        raise ValueError("Null values detected in required columns.")
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if not pd.api.types.is_numeric_dtype(df[col]):
            logger.error(f"Column '{col}' must be numeric.")
            raise TypeError(f"Column '{col}' must be numeric.")

def sanitize_symbol(symbol: str) -> str:
    """Sanitize symbol input to prevent path traversal or injection."""
    allowed = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789._-")
    sanitized = ''.join(c for c in symbol.upper() if c in allowed)
    if not sanitized:
        logger.error("Invalid or empty symbol after sanitization.")
        raise ValueError("Invalid or empty symbol after sanitization.")
    return sanitized

# --- Core Pipeline Functions ---

def load_data(symbol: str, interval: str = "1d") -> pd.DataFrame:
    """Load OHLCV data for a specific symbol and interval."""
    symbol = sanitize_symbol(symbol)
    interval = sanitize_symbol(interval)
    file_path = DATA_DIR / f"{symbol}_{interval}.csv"
    validate_file_path(file_path)
    try:
        df = pd.read_csv(file_path, parse_dates=["date"], index_col="date")
        validate_dataframe(df)
        logger.info(f"Loaded data for {symbol} ({interval}): {df.shape[0]} rows")
        return df
    except Exception as e:
        logger.exception(f"Failed to load data: {e}")
        raise

def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """Basic feature engineering on OHLCV data."""
    try:
        df = df.copy()
        df['returns'] = df['Close'].pct_change()
        df['volatility'] = df['returns'].rolling(window=5).std()
        df['sma_5'] = df['Close'].rolling(window=5).mean()
        df['sma_10'] = df['Close'].rolling(window=10).mean()
        df['target'] = (df['returns'].shift(-1) > 0).astype(int)
        df = df.dropna()
        if df.empty:
            logger.error("Feature engineering resulted in empty DataFrame.")
            raise ValueError("Feature engineering resulted in empty DataFrame.")
        logger.info(f"Feature engineering complete: {df.shape[0]} rows")
        return df
    except Exception as e:
        logger.exception(f"Feature engineering failed: {e}")
        raise

def train_model(df: pd.DataFrame) -> RandomForestClassifier:
    """Train a random forest classifier on OHLCV features."""
    try:
        for feat in FEATURES:
            if feat not in df.columns:
                logger.error(f"Missing feature column: {feat}")
                raise ValueError(f"Missing feature column: {feat}")
        X = df[FEATURES]
        y = df['target']
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        if len(X_train) == 0 or len(X_test) == 0:
            logger.error("Train/test split resulted in empty set.")
            raise ValueError("Train/test split resulted in empty set.")
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        train_score = accuracy_score(y_train, model.predict(X_train))
        test_score = accuracy_score(y_test, model.predict(X_test))
        logger.info(f"Train Accuracy: {train_score:.4f} | Test Accuracy: {test_score:.4f}")
        logger.info("Classification report:\n" + classification_report(y_test, model.predict(X_test)))
        logger.info("Confusion matrix:\n" + str(confusion_matrix(y_test, model.predict(X_test))))
        return model
    except Exception as e:
        logger.exception(f"Model training failed: {e}")
        raise

def save_model(model: RandomForestClassifier, symbol: str, interval: str = "1d") -> Path:
    """Save trained model to disk with metadata."""
    try:
        symbol = sanitize_symbol(symbol)
        interval = sanitize_symbol(interval)
        model_path = MODEL_DIR / f"{symbol}_{interval}_model.pkl"
        metadata = {
            "symbol": symbol,
            "interval": interval,
            "timestamp": pd.Timestamp.now().isoformat(),
            "model_type": "RandomForestClassifier"
        }
        joblib.dump({"model": model, "metadata": metadata}, model_path)
        logger.info(f"Model saved to {model_path}")
        return model_path
    except Exception as e:
        logger.exception(f"Failed to save model: {e}")
        raise

def run_training(symbol: str, interval: str = "1d") -> Optional[Path]:
    """Full training pipeline with robust error handling."""
    try:
        logger.info(f"Starting training pipeline for {symbol} ({interval})")
        df = load_data(symbol, interval)
        df = feature_engineering(df)
        model = train_model(df)
        model_path = save_model(model, symbol, interval)
        logger.info("Training pipeline completed successfully.")
        return model_path
    except Exception as e:
        logger.error(f"Training pipeline failed: {e}")
        return None

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train ML model on OHLCV data.")
    parser.add_argument("--symbol", type=str, required=True, help="Stock symbol (e.g., AAPL)")
    parser.add_argument("--interval", type=str, default="1d", help="Data interval (default: 1d)")
    args = parser.parse_args()

    run_training(symbol=args.symbol, interval=args.interval)
